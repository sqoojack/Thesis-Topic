# shadowcode.py
import torch
import torch.nn as nn
import itertools
import gc
from tqdm import tqdm
from config import Config
from utils import calc_fre_loss
from objectives import ObjectiveFactory

class ShadowCodeAttacker:
    def __init__(self, model, tokenizer, config: Config, lang):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.embedding_layer = model.get_input_embeddings()

    def run_alternating_attack(self, simulation):
        print(f"Starting Robust Alternating Attack on Target: {simulation.target_code.strip()}")
        
        # 準備基礎組件 (Suffix = Trigger)
        suffix_str = f"\n{simulation.position_code}"             
        suffix_ids = self.tokenizer(suffix_str, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)
        target_ids = self.tokenizer(simulation.target_code, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)
        
        candidate_pool = ObjectiveFactory.extract_candidate_tokens(simulation, self.tokenizer).to(self.config.DEVICE)
        print(f"Keyword Candidate Pool Size: {len(candidate_pool)} tokens")

        torch.manual_seed(self.config.SEED)
        perturbation_ids = torch.randint(0, self.model.config.vocab_size, (1, self.config.PERTURBATION_LEN)).to(self.config.DEVICE)
        keyword_ids = candidate_pool[torch.randperm(len(candidate_pool))[:self.config.KEYWORD_LEN]].unsqueeze(0)
        
        best_loss = float('inf')
        best_keyword_ids = keyword_ids.clone()
        best_perturbation_ids = perturbation_ids.clone()
        
        torch.cuda.empty_cache()
        
        for epoch in range(self.config.ALTERNATING_EPOCHS):
            print(f"\n=== Epoch {epoch + 1}/{self.config.ALTERNATING_EPOCHS} ===")
            
            # 1. 優化 Perturbation
            print(">> Optimizing Perturbation (Robust Gradient)...")
            perturbation_ids, p_loss = self._optimize_perturbation_robust(
                simulation, keyword_ids, perturbation_ids, suffix_ids, target_ids
            )
            print(f"   Best Perturbation Loss: {p_loss:.4f}")
            torch.cuda.empty_cache()
            
            # 2. 優化 Keyword (Grid Search)
            print(">> Optimizing Keyword (Grid Search)...")
            keyword_ids, k_loss = self._optimize_keyword_grid(
                simulation, candidate_pool, perturbation_ids, suffix_ids, target_ids
            )
            print(f"   Best Keyword Loss: {k_loss:.4f}")
            torch.cuda.empty_cache()
            
            if k_loss < best_loss:
                best_loss = k_loss
                best_keyword_ids = keyword_ids.clone()
                best_perturbation_ids = perturbation_ids.clone()
                print(f"   [New Best] Loss: {best_loss:.4f}")

        final_keyword = self.tokenizer.decode(best_keyword_ids[0], skip_special_tokens=True)
        final_perturbation = self.tokenizer.decode(best_perturbation_ids[0], skip_special_tokens=True)
        
        return final_keyword, final_perturbation

    def _get_comment_prefix_ids(self, lang):
        marker = "// " if lang in ["c", "cpp", "java"] else "# "
        return self.tokenizer(marker, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)

    def _optimize_perturbation_robust(self, simulation, keyword, perturbation, suffix, target):
        current_perturbation = perturbation.clone()
        embed_weights = self.embedding_layer.weight
        best_overall_loss = float('inf')
        
        NUM_GRAD_SAMPLES = 5 
        NUM_EVAL_SAMPLES = 1 
        
        cond_ids = self.tokenizer(simulation.conditional_code, return_tensors="pt", add_special_tokens=True).input_ids.to(self.config.DEVICE)
        
        # [Fix] 動態決定註解符號，避免 C 語言被當作 Preprocessor 指令 (#) 導致 Loss Inf
        comment_prefix_ids = self._get_comment_prefix_ids(simulation)
        
        pbar = tqdm(range(self.config.PERTURB_ITERATIONS), desc="Robust Iter", leave=False)
        
        for iteration in pbar:
            for token_idx in range(self.config.PERTURBATION_LEN):
                # --- 步驟 1: 計算梯度 ---
                total_grad = 0
                part1 = torch.cat([cond_ids, comment_prefix_ids, keyword], dim=1)
                
                for _ in range(NUM_GRAD_SAMPLES):
                    noise_str = simulation.get_noise_code()
                    noise_ids = self.tokenizer(noise_str, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)
                    if noise_ids.shape[1] > 256: noise_ids = noise_ids[:, :256] # 長度保護

                    part3 = torch.cat([noise_ids, suffix], dim=1)
                    grad, _ = self.token_gradients_with_suffix(part1, current_perturbation, part3, target)
                    total_grad += grad
                    del noise_ids, part3, grad
                
                avg_grad = total_grad / NUM_GRAD_SAMPLES
                del total_grad
                
                # --- 步驟 2: 評估候選 ---
                token_grad = avg_grad[:, token_idx, :]
                candidate_scores = -torch.matmul(token_grad, embed_weights.t())
                top_k_indices = torch.topk(candidate_scores, self.config.TOP_K, dim=1).indices[0]
                
                candidate_batch = current_perturbation.repeat(self.config.TOP_K, 1)
                candidate_batch[:, token_idx] = top_k_indices
                
                eval_noise_str = simulation.get_noise_code()
                eval_noise_ids = self.tokenizer(eval_noise_str, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)
                if eval_noise_ids.shape[1] > 256: eval_noise_ids = eval_noise_ids[:, :256]
                eval_part3 = torch.cat([eval_noise_ids, suffix], dim=1)
                
                current_loss = self.evaluate_batch_variable_component(
                    part1, current_perturbation, eval_part3, eval_part3, target, 
                    component="perturbation", show_progress=False, use_fre=True
                )[0].item()

                candidate_losses = self.evaluate_batch_variable_component(
                    part1, candidate_batch, eval_part3, eval_part3, target,
                    component="perturbation", show_progress=False, use_fre=True
                )
                
                min_loss_idx = torch.argmin(candidate_losses)
                min_loss = candidate_losses[min_loss_idx].item()
                
                if min_loss < current_loss:
                    current_perturbation = candidate_batch[min_loss_idx].unsqueeze(0)
                    if min_loss < best_overall_loss:
                        best_overall_loss = min_loss
                
                del avg_grad, candidate_batch, candidate_losses, eval_noise_ids, eval_part3
            
            pbar.set_postfix({'Best Loss': f'{best_overall_loss:.4f}'})

        return current_perturbation, best_overall_loss

    def _optimize_keyword_grid(self, simulation, candidate_pool, perturbation, suffix, target):
        pool_list = candidate_pool.tolist()
        combinations = list(itertools.product(pool_list, repeat=self.config.KEYWORD_LEN))
        candidate_keywords = torch.tensor(combinations, device=self.config.DEVICE)
        
        cond_ids = self.tokenizer(simulation.conditional_code, return_tensors="pt", add_special_tokens=True).input_ids.to(self.config.DEVICE)
        
        # [Fix] 同樣套用正確的註解符號
        comment_prefix_ids = self._get_comment_prefix_ids(simulation)
        
        noise_str = simulation.get_noise_code()
        noise_ids = self.tokenizer(noise_str, return_tensors="pt", add_special_tokens=False).input_ids.to(self.config.DEVICE)
        if noise_ids.shape[1] > 256: noise_ids = noise_ids[:, :256]
        
        part1 = torch.cat([cond_ids, comment_prefix_ids], dim=1) # [Cond] + [// ] or [# ]
        part3 = perturbation # [Pert]
        part4 = torch.cat([noise_ids, suffix], dim=1) # [Noise] + [Trigger]
        
        losses = self.evaluate_batch_variable_component(
            part1, candidate_keywords, part3, part4, target, 
            component="keyword", use_fre=True
        )
        
        best_idx = torch.argmin(losses)
        best_loss = losses[best_idx].item()
        best_keyword = candidate_keywords[best_idx].unsqueeze(0)
        
        del candidate_keywords, noise_ids, part4, losses
        return best_keyword, best_loss

    def evaluate_batch_variable_component(self, part1, variable_part, part3, part4, target, component="perturbation", show_progress=True, use_fre=False):
        batch_size = self.config.EVAL_BATCH_SIZE
        num_candidates = variable_part.shape[0]
        all_losses = []
        
        iterator = range(0, num_candidates, batch_size)
        if show_progress and num_candidates > batch_size:
            iterator = tqdm(iterator, desc=f"Eval {component}", leave=False)
        
        with torch.no_grad():
            for i in iterator:
                var_batch = variable_part[i : i + batch_size]
                curr_bs = var_batch.shape[0]
                
                p1 = part1.expand(curr_bs, -1)
                p3 = part3.expand(curr_bs, -1)
                tgt = target.expand(curr_bs, -1)
                
                if component == "keyword":
                    p4 = part4.expand(curr_bs, -1)
                    full_input = torch.cat([p1, var_batch, p3, p4, tgt], dim=1)
                else:
                    full_input = torch.cat([p1, var_batch, p3, tgt], dim=1)
                
                # MAX_LENGTH 截斷保護
                if full_input.shape[1] > self.config.MAX_LENGTH:
                    full_input = full_input[:, -self.config.MAX_LENGTH:]

                outputs = self.model(input_ids=full_input)
                logits = outputs.logits
                
                tgt_len = target.shape[1]
                target_logits = logits[:, -tgt_len-1 : -1, :] 
                
                if use_fre:
                    shift_logits = target_logits
                    shift_labels = tgt
                    loss_per_token = torch.nn.functional.cross_entropy(
                        shift_logits.transpose(1, 2), shift_labels, reduction='none'
                    )
                    l_origin = loss_per_token.mean(dim=1)
                    l_enhanced = loss_per_token[:, :min(self.config.FRE_H, loss_per_token.size(1))].mean(dim=1)
                    losses = (l_origin + self.config.FRE_R * l_enhanced) / (self.config.FRE_R + 1)
                else:
                    losses = torch.nn.functional.cross_entropy(
                        target_logits.reshape(-1, target_logits.size(-1)),
                        tgt.reshape(-1), reduction='none'
                    ).view(curr_bs, -1).mean(dim=1)
                
                all_losses.append(losses.cpu())
                del outputs, logits, target_logits, full_input
        
        return torch.cat(all_losses).to(self.config.DEVICE)

    def token_gradients_with_suffix(self, context_before, perturbation, suffix, target):
        full_input_ids = torch.cat([context_before, perturbation, suffix, target], dim=1)
        if full_input_ids.shape[1] > self.config.MAX_LENGTH:
            full_input_ids = full_input_ids[:, -self.config.MAX_LENGTH:]

        inputs_embeds = self.embedding_layer(full_input_ids).detach()
        inputs_embeds.requires_grad = True
        
        # 由於可能有截斷，需要重新計算偏移量
        p_len = perturbation.shape[1]
        p_end = full_input_ids.shape[1] - suffix.shape[1] - target.shape[1]
        p_start = p_end - p_len
        
        outputs = self.model(inputs_embeds=inputs_embeds)
        logits = outputs.logits
        
        tgt_len = target.shape[1]
        target_logits = logits[:, -tgt_len-1 : -1, :]
        loss = calc_fre_loss(target_logits, target, self.config)
        
        self.model.zero_grad()
        loss.backward()
        grad = inputs_embeds.grad[:, p_start:p_end, :]
        
        return grad, loss.item()